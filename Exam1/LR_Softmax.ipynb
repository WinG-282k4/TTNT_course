{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1bf0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 30\n",
      "X_test data shape: (30, 4)\n",
      "Number of predictions: 30\n",
      "Done writing predictions. Last sample number: 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Tránh tràn số\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Encode string labels to integer indices\n",
    "        self.classes_, y_indices = np.unique(y, return_inverse=True)\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        # Khởi tạo trọng số và bias\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "        self.bias = np.zeros(n_classes)\n",
    "        \n",
    "        # Mã hóa one-hot cho nhãn\n",
    "        y_one_hot = np.zeros((n_samples, n_classes))\n",
    "        for i in range(n_samples):\n",
    "            y_one_hot[i, y_indices[i]] = 1\n",
    "        \n",
    "        # Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # Tính xác suất Softmax\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.softmax(linear_model)\n",
    "            \n",
    "            # Tính gradient\n",
    "            grad_w = (1 / n_samples) * np.dot(X.T, (y_pred - y_one_hot))\n",
    "            grad_b = (1 / n_samples) * np.sum(y_pred - y_one_hot, axis=0)\n",
    "            \n",
    "            # Cập nhật tham số\n",
    "            self.weights -= self.lr * grad_w\n",
    "            self.bias -= self.lr * grad_b\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.softmax(linear_model)\n",
    "        class_indices = np.argmax(y_pred, axis=1)\n",
    "        # Convert indices back to original class labels\n",
    "        return self.classes_[class_indices]\n",
    "\n",
    "# Tải và tiền xử lý dữ liệu Iris\n",
    "iris = pd.read_csv(\"input_2.csv\")\n",
    "X, y = iris.iloc[:, :-1].values, iris.iloc[:, -1].values\n",
    "\n",
    "# Chuẩn hóa dữ liệu\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Đọc dữ liệu test từ file csv, đảm bảo đọc đúng format\n",
    "# Read first few lines to check if there's a header\n",
    "with open(\"output_2.csv\", \"r\") as f:\n",
    "    first_line = f.readline().strip()\n",
    "    # Check if the first line looks like a header or data\n",
    "    if ',' in first_line and all(c.isdigit() or c in '.,+-' for c in first_line.replace(',', '')):\n",
    "        # Looks like data, no header\n",
    "        X_test = pd.read_csv(\"output_2.csv\", header=None)\n",
    "    else:\n",
    "        # Has a header\n",
    "        X_test = pd.read_csv(\"output_2.csv\")\n",
    "\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "print(f\"X_test data shape: {X_test.shape}\")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model = SoftmaxRegression(lr=0.1, n_iters=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Dự đoán trên tất cả mẫu test\n",
    "# Chuẩn hóa dữ liệu test giống như dữ liệu train\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "# Dự đoán nhãn cho dữ liệu test\n",
    "predictions = model.predict(X_test_scaled)\n",
    "print(f\"Number of predictions: {len(predictions)}\")\n",
    "\n",
    "# Ghi kết quả ra file\n",
    "with open(\"iris_predictions.csv\", \"w\") as f:\n",
    "    f.write(\"Sample,Predicted_Label\\n\")\n",
    "    for i, pred in enumerate(predictions):\n",
    "        f.write(f\"{i+1},{pred}\\n\")\n",
    "\n",
    "print(\"Done writing predictions. Last sample number:\", len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0809a5",
   "metadata": {},
   "source": [
    "# Kiến trúc mạng Softmax Regression\n",
    "\n",
    "## 1. Tổng quan kiến trúc:\n",
    "- **Input Layer**: n_features đặc trưng đầu vào (4 đặc trưng cho dataset Iris)\n",
    "- **Linear Layer**: Ma trận trọng số W (4 x 3) + bias b (3)\n",
    "- **Softmax Activation**: Chuyển đổi logits thành xác suất\n",
    "- **Output Layer**: 3 lớp tương ứng với 3 loài hoa Iris\n",
    "\n",
    "## 2. Công thức toán học:\n",
    "```\n",
    "z = X @ W + b  # Linear transformation\n",
    "y_pred = softmax(z)  # Activation function\n",
    "```\n",
    "\n",
    "## 3. Kiến trúc chi tiết:\n",
    "```\n",
    "Input (150 samples x 4 features)\n",
    "    ↓\n",
    "Linear Layer: W(4x3) + b(3)\n",
    "    ↓  \n",
    "Logits (150 x 3)\n",
    "    ↓\n",
    "Softmax Activation\n",
    "    ↓\n",
    "Probabilities (150 x 3)\n",
    "    ↓\n",
    "Argmax → Predicted Class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8e3ce",
   "metadata": {},
   "source": [
    "# CÁC BƯỚC PHÂN LOẠI TRONG SOFTMAX REGRESSION\n",
    "\n",
    "## Bước 1: Chuẩn bị dữ liệu\n",
    "```python\n",
    "# Input: x = [sepal_length, sepal_width, petal_length, petal_width]\n",
    "# Ví dụ: x = [5.1, 3.5, 1.4, 0.2]\n",
    "```\n",
    "\n",
    "## Bước 2: Linear Transformation (Tính toán tuyến tính)\n",
    "```\n",
    "z = X @ W + b\n",
    "```\n",
    "- **X**: Ma trận input (n_samples × 4 features)\n",
    "- **W**: Ma trận trọng số (4 × 3 classes) \n",
    "- **b**: Vector bias (3,)\n",
    "- **z**: Vector logits (3,) - điểm số thô cho mỗi class\n",
    "\n",
    "## Bước 3: Softmax Activation (Chuyển đổi thành xác suất)\n",
    "```\n",
    "y_pred = softmax(z) = exp(z_i) / Σ(exp(z_j))\n",
    "```\n",
    "- Chuyển đổi logits thành xác suất [0,1]\n",
    "- Tổng các xác suất = 1\n",
    "- Class nào có xác suất cao nhất → Dự đoán\n",
    "\n",
    "## Bước 4: Prediction (Đưa ra dự đoán)\n",
    "```\n",
    "predicted_class = argmax(y_pred)\n",
    "```\n",
    "- Chọn class có xác suất cao nhất\n",
    "- Trả về tên class (setosa, versicolor, virginica)\n",
    "\n",
    "## Ví dụ cụ thể:\n",
    "```\n",
    "Input: [5.1, 3.5, 1.4, 0.2]\n",
    "↓\n",
    "Logits: [-2.1, 0.8, -1.3]\n",
    "↓\n",
    "Softmax: [0.05, 0.89, 0.06]\n",
    "↓\n",
    "Prediction: versicolor (index=1, confidence=89%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f07cf",
   "metadata": {},
   "source": [
    "# Giải thích hàm Softmax\n",
    "\n",
    "## Công thức Softmax:\n",
    "```\n",
    "softmax(z_i) = exp(z_i) / Σ(exp(z_j)) for j=1 to K\n",
    "```\n",
    "\n",
    "## Tại sao cần trừ max(z)?\n",
    "- **Vấn đề**: `exp(z)` có thể gây tràn số với z lớn\n",
    "- **Giải pháp**: `exp(z - max(z))` giữ nguyên tỷ lệ nhưng tránh tràn số\n",
    "- **Ví dụ**: \n",
    "  - z = [1000, 1001, 1002] → exp(z) = overflow!\n",
    "  - z - max(z) = [-2, -1, 0] → exp(z-max) = [0.135, 0.368, 1.0] ✓\n",
    "\n",
    "## Đặc tính của Softmax:\n",
    "1. **Tổng bằng 1**: Σ softmax(z_i) = 1\n",
    "2. **Giá trị [0,1]**: Mỗi output là xác suất hợp lệ\n",
    "3. **Phân biệt tốt**: Làm nổi bật class có logit cao nhất\n",
    "4. **Differentiable**: Có thể tính gradient cho training\n",
    "\n",
    "## Code implementation:\n",
    "```python\n",
    "def softmax(self, z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Tránh tràn số\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a47cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo chi tiết hàm Softmax với ví dụ cụ thể\n",
    "print(\"=== DEMO HÀM SOFTMAX ===\\n\")\n",
    "\n",
    "# Ví dụ với logits từ demo trên\n",
    "logits = np.array([[8.44541737, 3.78114046, -12.22655782]])\n",
    "print(f\"1. Logits ban đầu: {logits[0]}\")\n",
    "\n",
    "# Bước 1: Trừ max để tránh tràn số\n",
    "max_logit = np.max(logits, axis=1, keepdims=True)\n",
    "print(f\"2. Max logit: {max_logit[0][0]:.2f}\")\n",
    "\n",
    "stable_logits = logits - max_logit\n",
    "print(f\"3. Logits sau khi trừ max: {stable_logits[0]}\")\n",
    "\n",
    "# Bước 2: Tính exp\n",
    "exp_logits = np.exp(stable_logits)\n",
    "print(f\"4. exp(logits): {exp_logits[0]}\")\n",
    "\n",
    "# Bước 3: Tính tổng\n",
    "sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "print(f\"5. Tổng exp: {sum_exp[0][0]:.6f}\")\n",
    "\n",
    "# Bước 4: Chia để có xác suất\n",
    "softmax_probs = exp_logits / sum_exp\n",
    "print(f\"6. Xác suất Softmax: {softmax_probs[0]}\")\n",
    "print(f\"7. Tổng xác suất: {np.sum(softmax_probs[0]):.6f}\")\n",
    "\n",
    "# So sánh với hàm softmax của model\n",
    "model_softmax = model.softmax(logits)\n",
    "print(f\"\\n8. Kết quả từ hàm model: {model_softmax[0]}\")\n",
    "print(f\"9. Kiểm tra giống nhau: {np.allclose(softmax_probs, model_softmax)}\")\n",
    "\n",
    "print(\"\\n=== GIẢI THÍCH ===\\n\")\n",
    "print(\"• Trừ max(z) giúp tránh tràn số khi exp() với số lớn\")\n",
    "print(\"• Softmax biến đổi logits thành phân phối xác suất\")\n",
    "print(\"• Class có logit cao nhất sẽ có xác suất cao nhất\")\n",
    "print(\"• Tổng tất cả xác suất luôn bằng 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba94a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEMO QUÁ TRÌNH PHÂN LOẠI ===\n",
      "\n",
      "1. Input features: [-0.77168636  2.08046089 -1.80095818 -1.79182202]\n",
      "\n",
      "2. Linear output (logits): [  8.44541737   3.78114046 -12.22655782]\n",
      "   Shape: (1, 3)\n",
      "\n",
      "3. Softmax probabilities: [9.90661958e-01 9.33804069e-03 1.04279930e-09]\n",
      "   Sum of probabilities: 1.000000\n",
      "\n",
      "4. Predicted class index: 0\n",
      "   Predicted class name: Iris-setosa\n",
      "   Confidence: 0.9907\n",
      "\n",
      "=== CONFIDENCE CHO TẤT CẢ CLASSES ===\n",
      "Iris-setosa: 0.9907 ← PREDICTED\n",
      "Iris-versicolor: 0.0093 \n",
      "Iris-virginica: 0.0000 \n"
     ]
    }
   ],
   "source": [
    "# Demo quá trình phân loại với 1 mẫu cụ thể\n",
    "print(\"=== DEMO QUÁ TRÌNH PHÂN LOẠI ===\")\n",
    "\n",
    "# Lấy 1 mẫu từ dữ liệu test\n",
    "sample_idx = 0\n",
    "x_sample = X_test_scaled[sample_idx:sample_idx+1]  # Shape: (1, 4)\n",
    "print(f\"\\n1. Input features: {x_sample[0]}\")\n",
    "\n",
    "# Bước 1: Linear transformation\n",
    "linear_output = np.dot(x_sample, model.weights) + model.bias\n",
    "print(f\"\\n2. Linear output (logits): {linear_output[0]}\")\n",
    "print(f\"   Shape: {linear_output.shape}\")\n",
    "\n",
    "# Bước 2: Softmax activation  \n",
    "softmax_output = model.softmax(linear_output)\n",
    "print(f\"\\n3. Softmax probabilities: {softmax_output[0]}\")\n",
    "print(f\"   Sum of probabilities: {np.sum(softmax_output[0]):.6f}\")\n",
    "\n",
    "# Bước 3: Prediction (argmax)\n",
    "predicted_class_idx = np.argmax(softmax_output[0])\n",
    "predicted_class = model.classes_[predicted_class_idx]\n",
    "print(f\"\\n4. Predicted class index: {predicted_class_idx}\")\n",
    "print(f\"   Predicted class name: {predicted_class}\")\n",
    "print(f\"   Confidence: {softmax_output[0][predicted_class_idx]:.4f}\")\n",
    "\n",
    "# So sánh với các class khác\n",
    "print(\"\\n=== CONFIDENCE CHO TẤT CẢ CLASSES ===\")\n",
    "for i, class_name in enumerate(model.classes_):\n",
    "    prob = softmax_output[0][i]\n",
    "    status = \"← PREDICTED\" if i == predicted_class_idx else \"\"\n",
    "    print(f\"{class_name}: {prob:.4f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0d846",
   "metadata": {},
   "source": [
    "# Quá trình Training (Gradient Descent)\n",
    "\n",
    "## 1. Loss Function - Cross Entropy:\n",
    "```\n",
    "Loss = -Σ y_true * log(y_pred)\n",
    "```\n",
    "\n",
    "## 2. Gradient Descent:\n",
    "```python\n",
    "# Forward pass\n",
    "z = X @ W + b\n",
    "y_pred = softmax(z)\n",
    "\n",
    "# Compute gradients\n",
    "grad_W = (1/n) * X.T @ (y_pred - y_true)\n",
    "grad_b = (1/n) * Σ(y_pred - y_true)\n",
    "\n",
    "# Update parameters\n",
    "W = W - lr * grad_W\n",
    "b = b - lr * grad_b\n",
    "```\n",
    "\n",
    "## 3. One-hot Encoding:\n",
    "- **Input**: ['setosa', 'versicolor', 'virginica']\n",
    "- **One-hot**: [[1,0,0], [0,1,0], [0,0,1]]\n",
    "- **Mục đích**: Chuyển nhãn thành vector để tính loss\n",
    "\n",
    "## 4. Tại sao Softmax phù hợp cho multi-class?\n",
    "- **Output**: Phân phối xác suất trên tất cả classes\n",
    "- **Interpretable**: Mỗi giá trị là xác suất thuộc class đó\n",
    "- **Differentiable**: Có thể tính gradient để training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
